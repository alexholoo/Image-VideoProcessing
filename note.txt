Digital Image Processing:
Course: https://www.youtube.com/playlist?list=PLZ9qNFMHZ-A79y1StvUUqgyL-O0fZh2rs

Image and Video Processing,
	from Mars to Hollywood, with a stop at the hospital
	Guillermo Sapiro @Duke Univ.
  www.ImageProcessingPlace.com

compression
segementation
mars
consumer image and
	running and jumping, dispit the background, adobe

computer vision: identify all actions in the video. know what's happening

medical and biological images
	brain research
	HIV research

Chapter 2. Digital Image Fundamentals
Image Sampling & Quantization, Discrete,

3D info:
	Sensor array, e.g. 512*512, Pixel, AD conversion,
		Spatial quantization
		resolution: detail vs coarse
	Gray scale, 256,
		Gray value quantization
			e.g. 7/2*2 = 6
		gray level: 256, 128, 64, 32, 16, 8, 4, 2 (black & white)

Pro Cameras: RGB, Red, Green, Blue.
Consumer Camera s: Mosaic sensors & Interpolation
Images -> Video, ~30*3 images per second.

Week 2
vid 7 image compression (chapter 8)
why?
1000*1000 pixels * 24 bits * 30 frames/s * 60 s/min * 120 min/h => a 2h movie =>
very large number!!! => image & video compression

Redundancy: few colors, same color in regions, irrelevant info.

Image compression standards, formats and containers
	Image: JPEG, JPEG-LS, JPEG-2000, etc.
	Video: MPEG-1, MPEG-2, MPEG-4, etc.

image -> f(x,y) or f(x,y,t) -> Mapper (array) -> Quantizer (info loss)
-> Symbol coder (information theory, storage | transmission) =>
Symbol decoder -> Inverse mapper -> f`(x,y) or f`(x,y,t)

e.g. JPEG
Input image (M*N) -> Construct n*n subimages -> Forward transform -> Quantizer
-> Symbol encoder -> Compressed image
Compressed image -> symbol decoder -> inverse transform -> Merge n*n subimages
-> decompressed image


p008 Huffman coding
Picture Lina, frequency count, histogram, probability
Huffman coding
	Variable length code: higher the probability, lower the length
	prefix-free code
	theorem: shortest coding length on average
		compression ratio
		Entropy (H) = -SUM_symbols^(p(s)*log_2^(p(s)))
			average code length (Shannon's 1st theorem)


p009 JPEG 8*8 blocks
Input image (M*N) -> Construct n*n subimages -> Forward transform ->...
JPEG: 8*8 blocks
	JPEG is color blind
	RGB<-3*3 conversion matrix->YCbCr


p010 The discrete cosine transform (DCT)

Error: (root) Mean Square Error (MSE)

Transform n*n pixels subimages

Kahunen-Loeve Transform (KLT)
	note: transform is nothing but a multiplication by a matrix
	Put lots of info on the 1st coefficient, a bit more info in the 2ed, which
	is independent of the 1st, and so on...

Discrete Cosine Transform (DCT)
	basis functions n*n images
	decompose n*n subimage into a linear combination of these basis images
		- if the image is flat, then T(u=0,v=0) !=0, the others will be 0;
		- if the image has lots of variations, then u will have coefficients for
		other T(u,v)s. (kind of like in frequency domain in FFT)
	these basis images in DCT are constant, not in KLT (therefore expensive at
	runtime).

Why DCT?
1. In many cases a DCT is exactly equal to a KLT (neighboring pixels
are depending on each other in a particular form) Markovian
2. Boundary conditions: DCT has continuity between neighbor, while FFT
asks for periodicity.

Why 8*8?
1. computationally cheaper to do multiple small (n in n*n) DCT than to do one
large DCT.
2. The magic approximation of DCT to KLT occurs when the Markovian condition
applies. This condition applies only to regions that is not too small and also
not too big where there is not much correlation between pixels.


p011 Quantization
Quantize the coefficients, T(u,v), after the DCT.
	Uniform quantization
		simple
		used by JPEG
	Max-Lloyd Optimal Quantizer
		Error minimization: (s_i - t_i)^2
		Probability distribution


p012 JPEG-LS & MPEG
Predictive lossless compression
	predictor
	error image
	Haffman coding

Temperal prediction (MPEG)
	Motion vector, similar regions
